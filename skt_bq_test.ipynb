{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigquery_client():\n",
    "    import os\n",
    "    import tempfile\n",
    "    from google.cloud import bigquery\n",
    "    from skt.vault_utils import get_secrets\n",
    "    key = get_secrets('gcp/sktaic-datahub/dataflow')['config']\n",
    "    with tempfile.NamedTemporaryFile() as f:\n",
    "        f.write(key.encode())\n",
    "        f.seek(0)\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = f.name\n",
    "        client = bigquery.Client()\n",
    "    return client\n",
    "\n",
    "\n",
    "def bq_to_pandas(query):\n",
    "    bq = get_bigquery_client()\n",
    "    query_job = bq.query(query)\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "\n",
    "def get_spark_for_bigquery():\n",
    "    import os\n",
    "    import tempfile\n",
    "    from skt.vault_utils import get_secrets\n",
    "    from pyspark.sql import SparkSession\n",
    "    key = get_secrets('gcp/sktaic-datahub/dataflow')['config']\n",
    "    with tempfile.NamedTemporaryFile() as f:\n",
    "        f.write(key.encode())\n",
    "        f.seek(0)\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = f.name\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config('spark.driver.memory', '32g') \\\n",
    "            .config('spark.executor.memory', '8g') \\\n",
    "            .config('spark.shuffle.service.enabled', 'true') \\\n",
    "            .config('spark.dynamicAllocation.enabled', 'true') \\\n",
    "            .config('spark.dynamicAllocation.maxExecutors', '200') \\\n",
    "            .config('spark.driver.maxResultSize', '16g') \\\n",
    "            .config('spark.rpc.message.maxSize', '2000') \\\n",
    "            .config('spark.executor.memoryOverhead', '2000') \\\n",
    "            .config('spark.sql.execution.arrow.enabled', 'true') \\\n",
    "            .config(\"spark.jars\",\n",
    "                    \"/usr/hdp/3.0.1.0-187/spark2/jars/spark-bigquery-with-dependencies_2.11-0.13.1-beta.jar\") \\\n",
    "            .config('spark.yarn.queue', 'airflow_job') \\\n",
    "            .getOrCreate()\n",
    "        return spark\n",
    "\n",
    "\n",
    "def bq_table_to_df(dataset, table_name, col_list, partition=None, where=None):\n",
    "    spark = get_spark_for_bigquery()\n",
    "    df = spark.read.format(\"bigquery\") \\\n",
    "        .option(\"table\", f\"sktaic-datahub:{dataset}.{table_name}\") \\\n",
    "        .option(\"credentialsFile\", \"/etc/hadoop/conf/google-access-key.json\")\n",
    "    if partition:\n",
    "        table = get_bigquery_client().get_table(f'{dataset}.{table_name}')\n",
    "        if 'timePartitioning' in table._properties:\n",
    "            partition_column_name = table._properties['timePartitioning']['field']\n",
    "        elif 'rangePartitioning' in table._properties:\n",
    "            partition_column_name = table._properties['rangePartitioning']['field']\n",
    "        else:\n",
    "            partition_column_name = None\n",
    "        if partition_column_name:\n",
    "            filter = f'{partition_column_name} = {partition}'\n",
    "            df = df.option(\"filter\", filter)\n",
    "    df = df.load().select(col_list)\n",
    "    if where:\n",
    "        df.where(where)\n",
    "    return df.toPandas()\n",
    "\n",
    "\n",
    "def pandas_to_bq_table(df, dataset, table_name, partition=None):\n",
    "    spark = get_spark_for_bigquery()\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    table = f\"{dataset}.{table_name}${partition}\" if partition else f\"{dataset}.{table_name}\"\n",
    "    spark_df.write.format(\"bigquery\") \\\n",
    "        .option(\"credentialsFile\", \"/etc/hadoop/conf/google-access-key.json\") \\\n",
    "        .option(\"table\", table) \\\n",
    "        .option(\"temporaryGcsBucket\", \"mnoai-us\") \\\n",
    "        .save(mode='overwrite')\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = bq_table_to_df('ye_comm', 'device_meta', ['ym'], 2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_to_bq_table(r, 'ye_comm', 'device_meta_test', 2003)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
